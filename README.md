![Banner](https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Banner_GitHub_Feb2023.png?raw=true)
[![√ñffnen in Colab](https://img.shields.io/badge/Open%20in%20Colab-grey?style=flat-square&logo=google-colab&logoColor=F9AB00&labelColor=grey&color=F9AB00)](https://colab.research.google.com/drive/1halSc73m1BsovgJhM2FQkquQVUpD2_7B?usp=sharing)


Willkommen auf der GitHub Seite zu meiner **Bachelorarbeit**:  
**Entwicklung einer KI-basierten Objekterkennungsapplikation f√ºr einen Industrieroboter**.  

---

## üéØ Projekt√ºbersicht

In meiner Bachelorarbeit habe ich ein YOLOv8s-Modell trainiert, das speziell f√ºr die Objekterkennung von Baukl√∂tzen entwickelt wurde und auf dem Raspberry Pi 5 mit dem AI Kit sowie der Camera Module 3 ausf√ºhrbar ist.

Diese GitHub-Seite bietet:
- **Zugriff auf wichtige Dokumente, Codes und Ergebnisse**, um die Arbeit f√ºr Leser und Nutzer nachvollziehbar zu machen.
- **Detaillierte Repositories**, die Aspekte wie Modelltraining und Datensatzaufbereitung beleuchten.
---

### üìÇ Schritte der Projektumsetzung

1. Erstellen eines benutzerdefinierten Datensatzes mit Roboflow
2. Trainieren des Modells und Validierung der Ergebnisse
3. Ausf√ºhren des Modells auf dem Raspberry Pi 5
4. Konvertieren des Modells in das ONNX-Format
5. Konvertieren des ONNX-Modells in das HEF-Format
6. Ausf√ºhren des konvertierten Modells auf dem Raspberry Pi 5 & AI Kit
   
Diese Schritte umfassen den gesamten Entwicklungsprozess ‚Äì von der Datensammlung √ºber das Modelltraining bis hin zur finalen Anwendung auf der Hardware. Im Folgenden wird auf jede Projektphase detailliert eingegangen, um die Umsetzung und die verwendeten Methoden n√§her zu erl√§utern.

---

## üì∏ Erstellen eines benutzerdefinierten Datensatzes mit Roboflow
[Roboflow](https://roboflow.com/) bietet eine √ºbersichtliche Plattform zur Erstellung, Annotation (Abgrenzungen von Objekten in Bildern) und Verwaltung von Datens√§tzen. Mit √ºber 110.000 √∂ffentlich zug√§nglichen Datens√§tzen in [Roboflow Universe](https://universe.roboflow.com/) ist es ein vielseitiges Werkzeug f√ºr verschiedene Anwendungsbereiche. 

Im Folgenden wird gezeigt, wie ein benutzerdefinierter Datensatz erstellt werden kann.


### Schritt 1: Projekt erstellen:
Zum Erstellen eines neuen Projekts ist die [Registrierung bei Roboflow](https://app.roboflow.com/login) erforderlich. Nach der Registrierung kann ein Workspace benannt und ein neues Projekt im Dashboard angelegt werden. Die kostenlose Variante des Public Plans ist hierf√ºr ausreichend:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(10).gif?raw=true" alt="Demo" width="600">

### Schritt 2: Bilder hochladen:
Bilder k√∂nnen in das neu erstellte Projekt hochgeladen werden.
**Hinweis**: Wenn ein Datensatz bereits annotierte Dateien enth√§lt, k√∂nnen diese in Roboflow hochgeladen werden. Roboflow erkennt die Annotationen automatisch und ordnet sie den entsprechenden Bildern zu:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(6).gif?raw=true" alt="Demo" width="600">


### Schritt 3: Labeln & Annotation:

- **Manuelles Annotieren:**
Diese Methode bietet maximale Kontrolle √ºber die Pr√§zision der Annotationen. Objekte k√∂nnen im Bild markiert und mit passenden Labels versehen werden. Allerdings ist dieser Prozess zeitaufwendig:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(3).gif?raw=true" alt="Demo" width="600">

- **Auto-Labeling:**
Die Auto-Labeling-Funktion automatisiert den Annotierungsprozess, indem sie nach einem kurzen begleiteten Training, bei dem das Objekt der KI beschrieben wird, die Objekte im Bild erkennt und den restlichen Datensatz automatisch labelt. Diese Funktion ist besonders hilfreich, um Zeit zu sparen. Automatisch generierte Labels k√∂nnen dann √ºberpr√ºft und bei Bedarf angepasst werden:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(9).gif?raw=true" alt="Demo" width="600">

**Hinweis**: Nach dem Labeln und Annotieren wird die Aufteilung des Datensatzes in die drei Kategorien train, test und val abgefragt. Diese Trennung sollte best√§tigt werden, da sie f√ºr das sp√§tere Modelltraining von Bedeutung ist.

### Schritt 4: Neue Datensatzversion erstellen
Nach Abschluss der Annotationen kann eine neue Version des Datensatzes generiert werden. YOLO-Modelle werden in einem quadratischen Bildformat trainiert, weshalb der Datensatz in dieser Arbeit auf die Bildgr√∂√üe 640x640 skaliert wurde. Zus√§tzlich k√∂nnen Augmentationsmethoden wie Drehen, Skalieren oder das Anpassen von Helligkeit, Kontrast und S√§ttigung genutzt werden, um die Datenvielfalt und -robustheit zu erh√∂hen.

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(5).gif?raw=true" alt="Demo" width="600">

### Schritt 5: Datensatz exportieren
Der Datensatz steht nach der Generierung in verschiedenen Formaten, darunter im YOLOv8-Format, zum Export bereit. Der Datensatz sollte lokal auf dem Rechner gespeichert werden, da er f√ºr die sp√§tere Konvertierung ben√∂tigt wird, der Export kann auch √ºber die API direkt in eine Trainingsumgebung wie Google Colab integriert werden.

Im Beispiel wird gezeigt, wie mein finaler Datensatz, der f√ºr diese Arbeit erstellt wurde, in [Roboflow Universe](https://universe.roboflow.com/) gefunden und heruntergeladen werden kann.
Der Datensatz ist ebenfalls √ºber folgenden [Direktlink](https://www.google.com/url?q=https%3A%2F%2Funiverse.roboflow.com%2Fbauklotz%2Fbauklotz-c8zsq%2Fdataset%2F1) einsehbar:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(11).gif?raw=true" alt="Demo" width="600">

‚úî **Hinweis**: Nach dem Export wird empfohlen, den Datensatz erneut zu importieren, um automatisch generierte Annotationen zu √ºberpr√ºfen und bei Bedarf anzupassen.

---

## 2. **Trainieren des Modells und Validierung der Ergebnisse**

Das Modelltraining wird in [Google Colab](https://colab.research.google.com/drive/1halSc73m1BsovgJhM2FQkquQVUpD2_7B?usp=sharing) durchgef√ºhrt, um die Datei **best.pt** zu generieren. Dabei handelt es sich um eine PyTorch-Datei, die die optimierte Version des trainierten Modells enth√§lt und als Basis f√ºr die Modellausf√ºhrung auf dem Raspberry Pi 5 verwendet wird. √úber [diesen Link](https://colab.research.google.com/drive/1halSc73m1BsovgJhM2FQkquQVUpD2_7B?usp=sharing) kann das Modelltraining in Google Colab ausgef√ºhrt werden.

---

## 3. **Ausf√ºhren des Modells auf dem Raspberry Pi 5**
#### √úbertragen der Modell-Datei
Nach Erhalt der Datei **best.pt** kann diese √ºber Google Drive auf den Raspberry Pi 5 heruntergeladen werden.

#### Einrichten der virtuellen Umgebung
F√ºr die Modellausf√ºhrung ist zun√§chst eine virtuelle Umgebung erforderlich. Diese isoliert das Projekt, sodass es unabh√§ngig vom restlichen Betriebssystem und anderen installierten Paketen ausgef√ºhrt werden kann, ohne die Stabilit√§t des Systems zu gef√§hrden.

Mit folgendem Befehl wird eine neue virtuelle Umgebung namens **env** erstellt. Die Option **--system-site-packages** stellt sicher, dass die Umgebung Zugriff auf global installierte Python-Pakete hat:

```bash
python3 -m venv --system-site-packages env
```
```bash
source env/bin/activate
```

#### Systemaktualisierung und Paketinstallation
Das System wird aktualisiert, um die neuesten Paketlisten bereitzustellen und zuk√ºnftige Installationen reibungslos zu erm√∂glichen. Au√üerdem wird der Python-Paketmanager installiert und auf die neueste Version gebracht, um die Kompatibilit√§t mit aktuellen Bibliotheken sicherzustellen.

```bash
sudo apt update
sudo apt install python3-pip -y
pip install -U pip
```

#### Installation der YOLOv8-Bibliothek
Die YOLOv8-Bibliothek von Ultralytics wird zusammen mit allen f√ºr den Export erforderlichen Paketen installiert. Dies erm√∂glicht die Durchf√ºhrung der Objekterkennung mit YOLO auf dem Raspberry Pi:

```bash
pip install ultralytics[export]
```
#### Systemneustart

Ein Neustart des Systems stellt sicher, dass alle vorgenommenen √Ñnderungen und Installationen √ºbernommen wurden:

```bash
sudo reboot
```
### Nutzung von Thonny
Thonny ist eine benutzerfreundliche Entwicklungsumgebung f√ºr Python, die sich ideal f√ºr Projekte wie die Objekterkennung mit YOLO auf dem Raspberry Pi eignet. Sie erm√∂glicht das Arbeiten in virtuellen Umgebungen (venv), wodurch Skripte sicher getestet und ausgef√ºhrt werden k√∂nnen, ohne das Hauptsystem des Raspberry Pi zu beeintr√§chtigen. Dies erleichtert die Entwicklung direkt auf der Plattform. 
Innerhalb von Thonny kann die entsprechende virtuelle Umgebung ausgew√§hlt werden.

Im folgenden Video wird gezeigt, wie die Verkn√ºpfung von Thonny mit der venv eingerichtet wird:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20%26%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(12).gif?raw=true" alt="Demo" width="600">

#### Speicherort der virtuellen Umgebung und Ausf√ºhrung des Modells

Der Ordner der virtuellen Umgebung (venv) wird standardm√§√üig im aktuellen Arbeitsverzeichnis erstellt, in dem der entsprechende Befehl ausgef√ºhrt wurde. Auf dem Raspberry Pi befindet sich dieses Verzeichnis in der Regel im Home-Verzeichnis des Benutzers, beispielsweise unter **/home/robolab/**. Wird die virtuelle Umgebung mit dem Namen **env** erstellt, liegt diese anschlie√üend im Home-Verzeichnis unter **/home/robolab/env**, sofern kein alternatives Verzeichnis angegeben wurde.

Das folgende Skript sollte in die Benutzeroberfl√§che von Thonny eingef√ºgt werden. Es dient der Ausf√ºhrung des Modells (**best.pt**), das zuvor im Ordner der virtuellen Umgebung gespeichert wurde:


```python
import cv2
from picamera2 import Picamera2
from ultralytics import YOLO

# Kamera mit Picamera2 einrichten
picam2 = Picamera2()

# Bildgr√∂√üe und Format angeben (Gr√∂√üe kann hier angepasst werden, z. B. (640, 640))
picam2.preview_configuration.main.size = (640, 640)
picam2.preview_configuration.main.format = "RGB888"
picam2.preview_configuration.align()
picam2.configure("preview")
picam2.start()

# YOLOv8s-Modell laden
model = YOLO("best.pt") 

# Klassennamen definieren
class_names = {0: "Bauklotz"}  # Verkn√ºpft Klassenindizes mit Klassennamen

while True:
    # Einzelbild von der Kamera erfassen
    frame = picam2.capture_array()
    
    # YOLO-Modell auf das erfasste Bild anwenden und Ergebnisse speichern
    results = model(frame)
    detections = results[0].boxes.xyxy  # Extrahiere Koordinaten der Begrenzungsrahmen
    classes = results[0].boxes.cls     # Extrahiere Klassenindizes
    confidences = results[0].boxes.conf  # Extrahiere Konfidenzwerte

    # Zus√§tzliche Erkennungsdetails mit Koordinaten ausgeben
    print("\nAdditional Detection Details (with Coordinates):")
    for i, box in enumerate(detections):
        x1, y1, x2, y2 = map(int, box)  # Koordinaten als absolute Pixelwerte
        class_index = int(classes[i])
        class_name = class_names.get(class_index, "Unknown") # Klassennamen anhand des Index abrufen
        confidence = confidences[i] # Konfidenzwert f√ºr die aktuelle Erkennung

        # Erkennungsdetails ausgeben
        print(f"Object {i+1}: {class_name}, Confidence: {confidence:.2f}, Coordinates: ({x1}, {y1}, {x2}, {y2})")

        # Bounding  Box und Labels auf das Bild zeichnen
        label = f"{class_name} {confidence:.2f}"  # Klasse und Konfidenz anzeigen
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Bounding box (Begrenzungsrahmen)

        # Gr√∂√üe des Labels anpassen f√ºr bessere Lesbarkeit
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2, cv2.LINE_AA)

    # Bild mit den Ergebnissen anzeigen
    cv2.imshow("Camera", frame)

    # Programm beenden, wenn "q" gedr√ºckt wird
    if cv2.waitKey(1) == ord("q"):
        break

# Alle Fenster schlie√üen
cv2.destroyAllWindows()
```
Das folgende Bild zeigt die Ausgabe meines trainierten Modells. Die Objekte wurden erfolgreich erkannt und mit Bounding Boxen, Klassennamen sowie Konfidenzwerten versehen.


![image](https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/PT_modell.png?raw=true)



---

## 4. **Konvertieren des Modells in das ONNX-Format**

Das **ONNX-Format** (Open Neural Network Exchange) ist ein standardisiertes Austauschformat f√ºr neuronale Netzwerke, das die plattform√ºbergreifende Nutzung von Modellen erm√∂glicht. Es erlaubt den Export von Modellen aus Frameworks wie PyTorch oder TensorFlow und deren Verwendung in anderen Umgebungen.

F√ºr die Konvertierung ist die Definition einer **Opset-Version** (Operations Set) erforderlich. Opset legt fest, welche Funktionen und Operatoren innerhalb des Modells genutzt werden k√∂nnen. In dieser Arbeit wurde **Opset 9** verwendet, da es stabil und weit verbreitet ist sowie mit der HEF-Pipeline von Hailo kompatibel.

Die Konvertierung in das ONNX-Format erfolgt √ºber die YOLO CLI der Ultralytics-Pakete mit folgendem Befehl:

```bash
cd env
yolo export model=best.pt imgsz=640 format=onnx opset=9
```
Nach der erfolgreichen Ausf√ºhrung wird die ONNX-Datei im Arbeitsverzeichnis der virtuellen Umgebung gespeichert. Die Ausgabe der Konsole best√§tigt die erfolgreiche Konvertierung.

![Image](https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20%26%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/onnx_conversion.png)

---
## 5. **Konvertieren des ONNX-Modells in das HEF-Format**

### 1. Einrichtung einer Linux-Umgebung mit WSL und Ubuntu 22.04

F√ºr die Ausf√ºhrung eines trainierten Modells auf dem Raspberry Pi AI Kit mit der Hailo-Hardware ist eine Konvertierung des Modells in das **Hailo Execution Format** (HEF) erforderlich. Da die Hailo-Software ausschlie√ülich auf x86-Linux-Systemen unterst√ºtzt wird, bietet die Nutzung von **Windows Subsystem for Linux** (WSL) eine einfache L√∂sung f√ºr Windows-Nutzer.

Die Nutzung von WSL erm√∂glicht es, eine Linux-Umgebung direkt unter Windows einzurichten. Dies bietet die Vorteile einer isolierten Arbeitsumgebung und eines nahtlosen √úbergangs zwischen beiden Betriebssystemen. Die Ubuntu 22.04-Distribution kann entweder √ºber den Microsoft Store heruntergeladen oder direkt in der PowerShell installiert und verwendet werden.

<img src="https://github.com/user-attachments/assets/89f280d0-903b-4719-9b9d-d328ea8e20bd"  width="400">

#### Schritte zur Einrichtung der Linux-Umgebung:

#### 1. PowerShell √∂ffnen:
Die PowerShell auf dem Windows-System starten, um die notwendigen Befehle auszuf√ºhren.

#### 2. Verf√ºgbare Linux-Distributionen anzeigen:
Der folgende Befehl zeigt eine Liste der verf√ºgbaren Linux-Distributionen an:

```powershell
wsl --list --online
```

#### 3. Ubuntu 22.04 installieren:

Die gew√ºnschte Linux-Distribution (hier Ubuntu 22.04) wird mit folgendem Befehl installiert:


```powershell
wsl --install -d Ubuntu-22.04
```

#### 4. Benutzername und Passwort festlegen:
W√§hrend des Installationsprozesses wird ein Benutzername und ein Passwort f√ºr die zuk√ºnftige Verwendung mit **"sudo"**-Befehlen ben√∂tigt:

<img src="https://github.com/user-attachments/assets/bc178d9f-779e-49fa-befb-3e0e41385153" width="500">

#### 5. System aktualisieren:
Die Paketlisten werden aktualisiert und ein System-Upgrade durchgef√ºhrt, um sicherzustellen, dass alle Pakete auf dem neuesten Stand sind:

```powershell
sudo apt update
```
```powershell
sudo apt upgrade
```

#### Hinweise:

- Das Pr√§fix **sudo** (Superuser Do) erm√∂glicht die Ausf√ºhrung von Befehlen mit Administratorrechten. Dies ist erforderlich, um √Ñnderungen am System vorzunehmen.

- Nach der Installation und Einrichtung von Ubuntu wurde die Applikation genutzt, um die weiteren Schritte der Modellkonvertierung und Vorbereitung durchzuf√ºhren.


### 2. Hailo Data Compiler herunterladen 

Der Hailo Dataflow Compiler wird ben√∂tigt, um ONNX-Modelle in das HEF-Format (Hailo Execution Format) zu konvertieren, das f√ºr die Hailo-Hardware optimiert ist. Er passt Modelle an die Hardwarearchitektur an und erm√∂glicht durch Optimierungen wie Quantisierung eine maximale Leistung bei minimalem Ressourcenverbrauch.

#### Erforderliche Python-Pakete installieren
Die folgenden Befehle installieren die notwendigen Python-Pakete, die f√ºr den Hailo Data Compiler ben√∂tigt werden:

```bash
sudo apt install python3-pip
sudo apt install python3.10-venv
sudo apt-get install python3.10-dev python3.10-distutils python3-tk libfuse2 graphviz libgraphviz-dev
sudo pip install pygraphviz
sudo apt install wslu
```
#### Virtuelle Umgebung erstellen
Erstellen und aktivieren einer virtuellen Umgebung mit dem Namen **hailodfc**:

```bash
python3 -m venv hailodfc
source hailodfc/bin/activate
```
#### Python-Version √ºberpr√ºfen
Die Python-Version der Umgebung sollte 3.10.12 sein, um die Kompatibilit√§t sicherzustellen:
Um die Kompalit√§t mit der python Umgebung u sehen - Sie sollte 3.10.12 sein.

```bash
python3 --version
```
#### Hailo Dataflow Compiler herunterladen
F√ºr den Download des **Hailo Dataflow Compilers** ist eine [Registrierung in der Hailo Developer Zone](https://hailo.ai/authorization/?redirect_to=https%3A%2F%2Fhailo.ai%2Fdeveloper-zone%2Fsoftware-downloads%2) erforderlich. F√ºr die Konvertierung des Modells muss der Hailo Dataflow Compiler in der Version 3.28 heruntergeladen werden, da diese Version mit Python 3.10 kompatibel ist und die Anforderungen der Hailo-Hardware erf√ºllt.

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(13).gif?raw=true" alt="Demo" width="600">

#### Datei in das Ubuntu-Verzeichnis verschieben
Die heruntergeladene Hailo-DFC Datei sollte in das **Home-Verzeichnis** des Ubuntu-Umfelds kopiert werden. Mit folgendem Befehl wird das Zielverzeichnis ge√∂ffnet, um die Datei aus dem **Downloads-Ordner** zu verschieben:

```bash
wslview .
```
<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/dataflower.jpeg?raw=true"  width="500">

#### Erweiterung des Arbeitsspeichers mit Swap
Beim Installationsversuch erscheint folgende Fehlermeldung, wenn die erforderliche Kapazit√§t nicht erf√ºllt wird:

<img src="https://github.com/user-attachments/assets/560d98a5-de02-4fea-bdc6-96da151bcfd3" width="500">

In unserer WSL-Umgebung verwenden wir Ubuntu, da der Dataflow-Compiler mit den Versionen 20.04 und 22.04 kompatibel ist, besteht hier keine Komplikation.

Der Hailo Dataflow-Compiler ben√∂tigt mindestens 16 GB RAM, unsere Ubuntu-Umgebung stellt jedoch nur 8 GB zur Verf√ºgung. Um die Systemanforderungen zu erf√ºllen, wurde eine 8 GB Swap-Datei erstellt. Swap ist ein Bereich auf der Festplatte oder SSD, der als virtueller Arbeitsspeicher verwendet wird, wenn der physische RAM ausgelastet ist. Zwar ist Swap langsamer als RAM, erm√∂glicht aber die Kompilierung und Optimierung von Modellen, ohne die Hardware aufzur√ºsten.

![image](https://github.com/user-attachments/assets/9c2a742d-a15e-4a4c-84f1-55024377d212)

Eine neue Swap-Datei mit 9 GB wird eingerichtet, um die Systemanforderungen des Hailo Dataflow-Compilers zu erf√ºllen. Nach Erstellung der Datei sollte keine Fehlermeldung mehr auftreten:

```bash
sudo fallocate -l 9G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

Mit dem folgenden Befehl kann √ºberpr√ºft werden, ob der Swap-Speicher korrekt eingerichtet wurde:

```bash
free -h
cat /proc/swaps
```
Das Ergebnis sollte wie folgt aussehen:
![image](https://github.com/user-attachments/assets/ee224632-7bc5-4df2-b457-5e0a1115eb1b)


#### Installation starten
Nun kann die installation des Hailo-DFC gestartet werden:

```bash
pip3 install hailo_dataflow_compiler-3.28.0-py3-none-linux_x86_64.whl
```
#### Installation √ºberpr√ºfen
Der Befehl **-h** zeigt nach erfolgreicher Installation eine √úbersicht der verf√ºgbaren Optionen und Befehle, die mit der Hailo-CLI ausgef√ºhrt werden k√∂nnen.

```bash
hailo -h
```
![image](https://github.com/user-attachments/assets/990df005-46d5-4626-979c-c5db122b57f1)


### 3. Hailo Model Zoo herunterladen

Der **Hailo Model Zoo** spielt eine zentrale Rolle im Workflow f√ºr die Konvertierung von Modellen in das Hailo Execution Format (HEF). Er enth√§lt eine Sammlung vorgefertigter Modelle und Skripte, die speziell f√ºr die Hailo-Hardware optimiert sind. Diese Ressourcen erm√∂glichen es, Modelle effizient anzupassen, zu konvertieren und f√ºr die Ausf√ºhrung auf der Hailo-Hardware vorzubereiten.

Um den Hailo Model Zoo herunterzuladen, wird das offizielle **GitHub-Repository** von Hailo verwendet:

```bash
git clone https://github.com/hailo-ai/hailo_model_zoo.git
```

Nach dem Herunterladen muss in das entsprechende Verzeichnis gewechselt werden, um die darin enthaltenen Pakete zu installieren, die f√ºr das Setup und die Konvertierung erforderlich sind:

```bash
cd hailo_model_zoo; pip install -e .

```

## 6. Vorbereitung auf die HEF Konverterierung 

Um die HEF-Konvertierung vorzubereiten, m√ºssen mehrere Dateien und Skripte angepasst und in das richtige Verzeichnis verschoben werden. Hier eine √úbersicht der erforderlichen Schritte:

### 1. best.onnx Datei √ºbertragen

Die zuvor generierte **best.onnx**-Datei wird vom Raspberry Pi √ºber Google Drive heruntergeladen und anschlie√üend in das Home-Verzeichnis von Ubuntu verschoben.  Dieser Schritt stellt sicher, dass alle ben√∂tigten Ressourcen an einem Ort f√ºr die Konvertierung bereitstehen:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/best.jpeg?raw=true"  width="500">

### 2. Datensatz vorbereiten
Beim Herunterladen des Datensatzes wurden Bilder und Annotationen in drei Ordner unterteilt: **train**, **val** und **test**. Der train-Ordner muss ebenfalls in das Ubuntu-Umfeld √ºbertragen werden, um f√ºr die Konvertierung verf√ºgbar zu sein:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/trsin_folder.jpeg?raw=true" width="400">

### 3. Konfigurationsdateien im Hailo Model Zoo anpassen
Viele machen den Fehler, bereits jetzt den Befehl zur Performance-Optimierung auszuf√ºhren, ohne sicherzustellen, dass alle Konfigurationsdateien korrekt angepasst sind. 

Es m√ºssen folgende Skripte erst angepasst werden:

#### a.) yolov8s.yaml
Diese Datei ist eine zentrale Konfigurationsdatei des Hailo Model Zoo und definiert alle Parameter f√ºr das Parsing, die Optimierung und die Kompilierung des Modells. Sie befindet sich im Verzeichnis: **Ubuntu-22.04 > home > irep > hailo_model_zoo > cfg > networks.**

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/network.jpeg?raw=true" width="500">

In der Konfigurationsdatei **yolov8s.yaml** sind die folgenden √Ñnderungen vorzunehmen. Kommentare im Code weisen auf die anzupassenden Parameter hin, um die Konfiguration auf die Projektanforderungen abzustimmen:

```yaml
base:
- base/yolov8.yaml
postprocessing:
  device_pre_post_layers:
    nms: true
  hpp: true
network:
  network_name: yolov8s
paths:
  network_path:
  - best.onnx #Hier muss der genaue Pfad zur ONNX Datei angegeben werden 
  alls_script: hailo_model_zoo/hailo_model_zoo/cfg/alls/yolov8s.alls #Pfad zur ONNX-Datei angeben, die im vorherigen Schritt generiert wurde.

  url: #URL leer stehen lassen
parser:
  nodes:
  - null
  - - /model.22/cv2.0/cv2.0.2/Conv
    - /model.22/cv3.0/cv3.0.2/Conv
    - /model.22/cv2.1/cv2.1.2/Conv
    - /model.22/cv3.1/cv3.1.2/Conv
    - /model.22/cv2.2/cv2.2.2/Conv
    - /model.22/cv3.2/cv3.2.2/Conv
info:
  task: object detection
  input_shape: 640x640x3
  output_shape: 1x5x100 #Die erste Zahl gibt die Anzahl der erkannten Klassen an (1=Bauklotz).
  operations: 28.4G # GFLOPS entspricht der Angabe in der "Model Summary (fused)" aus dem Modelltraining in Google Colab
  parameters: 11.125M # Parameteranzahl basierend auf der 'Model Summary (fused)' aus Google Colab
  framework: pytorch
  training_data: coco train2017
  validation_data: coco val2017
  eval_metric: mAP
  full_precision_result: 44.75
  source: https://github.com/ultralytics/ultralytics
  license_url: https://github.com/ultralytics/ultralytics/blob/main/LICENSE
  license_name: GPL-3.0

```
Die **Parameteranzahl** und die **GFLOPS** (Giga Floating Point Operations per Second) des Modells k√∂nnen direkt aus den Ergebnissen des Modelltrainings in Google Colab abgelesen werden:

![image](https://github.com/user-attachments/assets/28d0eb7f-9862-49c5-9f83-c0ae7ab9c664)



#### b.) yolov8s.alls

definiert spezifische Parameter und Einstellungen f√ºr die Optimierung und Konvertierung des Modells, einschlie√ülich der Aktivierungsfunktionen (z. B. Sigmoid).
Speicherort: **Ubuntu-22.04 > home > irep > hailo_model_zoo > cfg > alls > generic.**

![image](https://github.com/user-attachments/assets/2905ce2d-8423-4528-ace4-11579f15bf11)

In der Konfigurationsdatei **yolov8s.alls** sind die folgenden √Ñnderungen vorzunehmen. Kommentare im Code weisen auf die anzupassenden Parameter hin, um die Konfiguration auf die Projektanforderungen abzustimmen:

```plaintext
quantization_param([conv42, conv53, conv63], force_range_out=[0.0, 1.0]) 
normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])
change_output_activation(conv42, sigmoid)
change_output_activation(conv53, sigmoid)
change_output_activation(conv63, sigmoid)
performance_param(compiler_optimization_level=max) #Zeile erg√§nzen
nms_postprocess("../../postprocess_config/yolov8s_nms_config.json", meta_arch=yolov8, engine=cpu)
```

#### c.) yolov8s_nms_config.json

Die yolov8s_nms_config.json-Datei legt die Parameter f√ºr das NMS (Non-Maximum Suppression)-Postprocessing fest. Sie wird vom Dataflow Compiler genutzt, um die Nachbearbeitungsschritte f√ºr das Netzwerk zu konfigurieren. W√§hrend die voreingestellten Werte f√ºr viele Netzwerke geeignet sind, ist eine Anpassung notwendig, wenn sich Hyperparameter √§ndern. Speicherort: **Ubuntu-22.04 > home > irep > hailo_model_zoo > cfg > postprocess_config**

![image](https://github.com/user-attachments/assets/4172ebb3-c527-4208-8819-20472d413967)

In der Konfigurationsdatei **yolov8s_nms_config.json** sind die folgenden √Ñnderungen vorzunehmen:

- **"image_dims": [640, 640]** // Bilddimensionen an die Trainingsparameter des Modells anpassen 
- **"classes": 1** // Klassenanzahl entsprechend des eigenen Modells anpassen (hier: 1 f√ºr Bauklotz)


```json
{
	"nms_scores_th": 0.2,
	"nms_iou_th": 0.7,
	"image_dims": [
		640,
		640 
	],
	"max_proposals_per_class": 100,
	"classes": 1, 
	"regression_length": 16,
	"background_removal": false,
	"background_removal_index": 0,
	"bbox_decoders": [
		{
			"name": "bbox_decoder41",
			"stride": 8,
			"reg_layer": "conv41",
			"cls_layer": "conv42"
		},
		{
			"name": "bbox_decoder52",
			"stride": 16,
			"reg_layer": "conv52",
			"cls_layer": "conv53"
		},
		{
			"name": "bbox_decoder62",
			"stride": 32,
			"reg_layer": "conv62",
			"cls_layer": "conv63"
		}
	]
}
```


Dieser Befehl startet die Optimierung. **Hinweis:** Die Optimierung ist zeitaufw√§ndig und kann je nach Leistung des Systems bis zu vier Stunden dauern.

```bash
hailomz compile yolov8s --ckpt=best.onnx --hw-arch hailo8l --calib-path train/images --classes 1 --performance
```
Nach erfolgreicher Optimierung wird die HEF-Datei im angegebenen Verzeichnis gespeichert.
![image](https://github.com/user-attachments/assets/7183d63d-2382-4a96-a527-8c46464e1d64)

---
## 7. **Modellausf√ºhrung auf dem Raspberry PI 5 & Ai KIT**

### PCIe auf Gen3 einstellen und AI-Kit-Platine montieren

PCIe (Peripheral Component Interconnect Express) ist ein Hochgeschwindigkeits-Standardschnittstellenprotokoll, das eine schnelle Daten√ºbertragung zwischen der Hauptplatine und angeschlossenen Ger√§ten wie dem Hailo AI-Kit erm√∂glicht. Um die optimale Leistung zu erzielen, ist es wichtig, PCIe auf Gen3 (Generation 3) zu konfigurieren.

#### Schritt 1: Raspberry AI Kit montieren
Befestige das Raspberry AI Kit sicher auf deinem Raspberry Pi, indem du die GPIO-Pins korrekt ausrichtest. Stelle sicher, dass die Platine fest sitzt, um eine stabile Verbindung zu gew√§hrleisten, hier ein Tutorial dazu:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Raspberry_Ai_Kit.gif?raw=true" alt="Demo" width="600">

#### Schritt 2: PCIe auf Gen3 konfigurieren
PCIe (Peripheral Component Interconnect Express) ist ein Hochgeschwindigkeits-Standardschnittstellenprotokoll, das h√§ufig in Computern verwendet wird, um Komponenten wie Grafikkarten, SSDs oder spezielle Beschleuniger wie das Hailo-Ger√§t anzuschlie√üen. Es erm√∂glicht eine schnelle Daten√ºbertragung zwischen diesen Ger√§ten und der Hauptplatine. Um die optimale Leistung des Hailo-Ger√§ts zu erzielen, muss PCIe auf Gen3 (Generation 3) gesetzt werden. Die Nutzung von Gen2 (Generation 2) ist zwar m√∂glich, f√ºhrt jedoch zu einer geringeren Leistung aufgrund der niedrigeren √úbertragungsgeschwindigkeit.

√ñffne daf√ºr das Konfigurationstool des Raspberry Pi:
```bash
sudo raspi-config
```
W√§hle die Option "6 Advanced Options" und danach "A8 PCIe Speed". Best√§tige mit "Yes", um den PCIe-Gen3-Modus zu aktivieren. W√§hle anschlie√üend "Finish", um das Tool zu verlassen.
Starte den Raspberry Pi neu:

```bash
sudo reboot
```

### Einrichtung der Umgebung

F√ºr die Ausf√ºhrung des Modells muss eine geeignete Arbeitsumgebung eingerichtet werden. Dazu wird das **hailo-rpi5-Examples**-Repository auf den Raspberry Pi heruntergeladen:

```bash
git clone https://github.com/hailo-ai/hailo-rpi5-examples.git
```
```bash
cd hailo-rpi5-examples
```
```bash
./install.sh
```
Sollte das Skript **./install.sh** nicht wie vorgesehen funktionieren und das Herunterladen von Paketen scheitern, gibt es eine alternative Methode. Die ben√∂tigten Pakete k√∂nnen manuell installiert werden, indem eine virtuelle Umgebung wie folgt eingerichtet wird:

```bash
source setup_env.sh
```
```bash
pip install -r requirements.txt
```
```bash
./download_resources.sh
```
```bash
./compile_postprocess.sh
```

Nach dem Schlie√üen der virtuellen Umgebung **venv_hailo_rpi5_examples** muss f√ºr jede neue Terminalsitzung mit den installierten Paketen stets das Skript **source setup_env.sh** ausgef√ºhrt werden.

**Hinweis:** Das Verzeichnis hailo-rpi5-examples muss vor der Ausf√ºhrung als aktuelles Arbeitsverzeichnis gesetzt sein, um Zugriff auf die installierten Pakete zu gew√§hrleisten:

```bash
cd hailo-rpi5-examples
```

```bash
source setup_env.sh
```
Die venv kann durch den Befehl **deactivate** geschlossen werden:

```bash
deactivate
```
### GitHub-Repository und Beispiele

Das GitHub-Repository **hailo-rpi5-examples** bietet verschiedene Beispiele zur Nutzung des Hailo AI Kits mit dem Raspberry Pi 5, einschlie√ülich der aktuell genutzten Objekterkennungsfunktion. Dar√ºber hinaus stehen weitere Beispiele zur Verf√ºgung, die die vielseitigen Einsatzm√∂glichkeiten des Hailo AI Kits demonstrieren. Mehr Informationen und Anleitungen sind auf der [GitHub-Seite](https://github.com/hailo-ai/hailo-rpi5-examples/tree/main) zu finden.


### Vorbereitung der Dateien

Um die generierte HEF Datei nutzen zu k√∂nnen, m√ºssen folgende Dokumente im hailo-rpi5-examples Ordner in den folgenden Verzechnissen hinterlegt werden: 

#### Ressources Ordner:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/ressources.jpeg?raw=true" width="500">

Im Verzeichnis **resources** werden die JSON-Datei und die HEF-Datei abgelegt. Die JSON-Datei definiert zentrale Parameter f√ºr die Modellausf√ºhrung. Als Orientierung dient die bereits im Ordner hinterlegte **barcode-json**-Datei. Basierend darauf wurde die folgende **Bauklotz-json** -Datei erstellt:

```json
{
    "iou_threshold": 0.4,
    "detection_threshold": 0.5,
    "output_activation": 0.5,
    "label_offset": 0.5,
    "max_boxes":3,
    "labels": [
      "unlabeled",
      "Bauklotz"
    ]
}
```
##### Paramter√ºbersicht:

- **iou_threshold (0.4)**: Legt fest, dass Bounding Boxen mit mehr als 40 % √úberlappung zusammengef√ºhrt werden.

- **detection_threshold: (0.5)** Objekte werden nur erkannt, wenn die Konfidenz mindestens 50 % betr√§gt.

- **max_boxes (3)**: Begrenzung der maximalen Anzahl anzuzeigender Objekte auf drei. Die Labels definieren die Klassen ‚Äûunlabeled‚Äú und ‚ÄûBauklotz‚Äú.

#### Basic-pipeline - Ordner:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/pipeline.jpeg?raw=true" width="400">

im Verzeichnis basic_pipeline werden Skripte f√ºr die Modellausf√ºhrung abgelegt. Dazu z√§hlen:

- **detection_pipeline.py** und **detection.py**: Standardm√§√üig im Repository enthalten. Diese Skripte k√∂nnen an spezifische Anforderungen angepasst werden.
  
- **Bauklotz_detection.py:** Mein angepasstes Skript, das Bounding Boxen um erkannte Objekte zeichnet und relevante Informationen wie Klasse, Konfidenz und Verarbeitungsgeschwindigkeit anzeigt.

Das folgende Skript (Bauklotz_detection.py) wird verwendet:

```python
import gi
gi.require_version('Gst', '1.0')
from gi.repository import Gst, GLib
import os
import numpy as np
import cv2
import hailo
from hailo_rpi_common import (
    get_caps_from_pad,
    get_numpy_from_buffer,
    app_callback_class,
)
from detection_pipeline import GStreamerDetectionApp
import time

# User-defined class for the callback function
class user_app_callback_class(app_callback_class):
    def __init__(self):
        super().__init__()
        self.new_variable = 42


# Callback function for the pipeline
def app_callback(pad, info, user_data):
    buffer = info.get_buffer()
    if buffer is None:
        return Gst.PadProbeReturn.OK

    user_data.increment()
    string_to_print = f"Frame count: {user_data.get_count()}\n"

    # Get camera resolution
    format, width, height = get_caps_from_pad(pad)
    print(f"Camera Resolution: {width}x{height}")

    frame = None
    if user_data.use_frame and format is not None:
        frame = get_numpy_from_buffer(buffer, format, width, height)
        frame = cv2.resize(frame, (640, 640))  # Resize for display

    roi = hailo.get_roi_from_buffer(buffer)
    detections = roi.get_objects_typed(hailo.HAILO_DETECTION)

    detection_count = 0

    # Start timing
    start_time = time.time()

    for detection in detections:
        label = detection.get_label()
        bbox = detection.get_bbox()
        confidence = detection.get_confidence()

        # Convert normalized to absolute coordinates
        xmin = int(bbox.xmin() * width)
        ymin = int(bbox.ymin() * height)
        xmax = int(bbox.xmax() * width)
        ymax = int(bbox.ymax() * height)

        # Update the shell output to match (xmin, ymin, xmax, ymax)
        string_to_print += f"Detection: {label}, Confidence: {confidence:.2f}, BBox: ({xmin}, {ymin}, {xmax}, {ymax})\n"

        # Draw bounding box and class+confidence as percentage on the frame
        if user_data.use_frame and frame is not None:
            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
            # Display label, confidence, and bounding box format
            cv2.putText(frame, f"{label} {int(confidence * 100)}%", (xmin, ymin - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        detection_count += 1

    # Measure timing for detection pipeline
    elapsed_time = (time.time() - start_time) * 1000  # Convert to milliseconds

    string_to_print += f"Speed: {elapsed_time:.2f}ms per frame\n"
    print(string_to_print)

    if user_data.use_frame and frame is not None:
        # Display the number of detections
        cv2.putText(frame, f"Detections: {detection_count}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        user_data.set_frame(frame)

    return Gst.PadProbeReturn.OK

if __name__ == "__main__":
    user_data = user_app_callback_class()
    app = GStreamerDetectionApp(app_callback, user_data)
    app.run()
```


### Ausf√ºhren der Ausgabe:

Zum Starten des Modells wird der folgende Befehl ausgef√ºhrt:

```bash
python basic_pipelines/bauklotz_detection.py --labels-json resources/bauklotz-labels.json --hef resources/yolov8s-hailo8l-barcode.hef --i -rpi

```

Der Parameter **--input rpi** gibt an, dass die angeschlossene Raspberry Pi Camera Module 3 Kamera als Eingabequelle verwendet wird. Um weitere Optionen oder Ausgaben, die mit dem Skript detection.py verf√ºgbar sind, zu erkunden, kann folgender Befehl ausgef√ºhrt werden:

```bash
python basic_pipelines/detection.py --help
```

Die folgende Abbildung zeigt die Ausgabe des Modells. Die Objekte wurden erfolgreich erkannt und mit Bounding Boxen, Klassen und den jeweiligen Konfidenzwerten versehen:

![image](https://github.com/user-attachments/assets/6c91a661-7dde-479a-90b4-65f2520f9534)

---

## üåü Warum wurde eine GitHub Seite erstellt?

Die GitHub-Seite wurde erstellt, um den Code, die Konfigurationen und die Dokumentationen des Projekts zentral zu verwalten und zug√§nglich zu machen. 

W√§hrend der Arbeit an diesem Projekt haben andere GitHub-Seiten wertvolle Unterst√ºtzung geboten, um komplexe Schritte besser zu verstehen und umzusetzen. Aus diesem Grund habe ich beschlossen, f√ºr die Bachelorarbeit ebenfalls eine GitHub-Seite zu erstellen, um die Projektschritte √ºbersichtlich zu dokumentieren und mit Nachfolgenden zu teilen. 

Zudem ist diese Seite eine der ersten deutschen Ressourcen, die sich detailliert mit diesem Thema auseinandersetzt.

---

## üìñ Quellen, die f√ºr diese Seite genutzt wurden, befinden sich im Repository unter 'Quellen'


